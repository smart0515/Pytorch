### 🧐 파이토치 문법

- **PyTorch 패키지의 구성 요소**
    - `torch`
        - main namespace로 tensor등의 다양한 수학 함수가 패키지에 포함되어 있습니다.
        - **NumPy와 같은 구조**를 가지고 있어서 numpy와 상당히 비슷한 문법 구조를 가지고 있습니다.
    - `torch.autograd`
        - 자동 미분을 위한 함수가 포함되어 있습니다.
        - 자동 미분의 on, off를 제어하는 enable_grad 또는 no_grad나 자체 미분 가능 함수를 정의할 때 사용하는 기반 클래스인 Function등이 포함됩니다.
    - `torch.nn`
        - 신경망을 구축하기 위한 다양한 데이터 구조나 레이어가 정의되어 있습니다.
        - CNN, LSTM, 활성화 함수(ReLu), loss 등이 정의되어 있습니다.
    - `torch.optim`
        - SGD 등의 파라미터 최적화 알고리즘 등이 구현되어 있습니다.
    - `torch.utils.data`
        - Gradient Descent 계열의 반복 연산을 할 때, 사용하는 미니 배치용 유틸리티 함수가 포함되어 있습니다.
    - `torch.onnx`
        - ONNX(Open Neural Network eXchange) 포맷으로 모델을 export 할 때 사용합니다.
        - ONNX는 서로 다른 딥러닝 프레임워크 간에 모델을 공유할 때 사용하는 새로운 포맷입니다.

**optim.zero_grad()**    #기울기 초기화 (반드시 0으로 초기화시킨 후 학습을 진행해야 한다.)
**loss.backward()**        #기울기 계산
**optim.step()**              #weight 업데이트

이 셋은 세트이다.(data_loader를 돌리는 for문 안에서 매번 해줘야 한다)
train 모드에서는 3개 다 해야하고 val 모드에서는 optim.zero_grad()만 해야 함

**.detach()** : 내용물은 같지만 require_grad=False가 된다.

**transforms.Compose()** : 리스트 안에 순서대로 전처리 진행

**transforms.ToTensor()** : Numpy 또는 PIL 자료형을 Tensor 타입으로 변경해줌

### 텐서의 차원 조작

- 텐서의 차원을 변경하는 `view`나 텐서를 결합하는 `stack`, `cat`, 차원을 교환하는 `t`, `transpose`도 사용됩니다.
- `view`는 numpy의 reshape와 유사합니다. 물론 pytorch에도 `reshape` 기능이 있으므로 `view`를 사용하던지 `reshape`을 사용하던지 사용방법은 같으므로 선택해서 사용하면 됩니다. (**reshape를 사용하길 권장합니다.**)

`**batch size` `iteration` `epoch`**

- 보통 학습을 할 때에는 GPU 메모리의 한계로 인하여 한번에 GPU를 통해 연산되는 데이터 양이 제한적입니다. 예를 들어 데이터가 총 100개가 있으면 20개씩 데이터를 분할하여 5번 나눠서 학습을 하곤 합니다. 이 때, 20개라는 데이터의 크기를 `batch size`라고 합니다. 그리고 5번 이라는 나눠서 학습하는 횟수를 `iteration`이라고 합니다. 따라서 `batch size * iteration`을 하면 현재 가지고 있는 데이터 전체를 대상으로 학습을 하게 됩니다. 전체 데이터를 학습한 단위를 `epoch`이라고 합니다. 10 epoch을 학습하였다는 뜻은 100개의 데이터를 10번 반복학습 하였다는 뜻입니다.

### scheduler

```python
# model → optimizer → scheduler
model = Net().to(device)

learning_rate = 0.01
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

from torch.optim.lr_scheduler import ReduceLROnPlateau
scheduler = ReduceLROnPlateau(optimizer, mode = 'max', factor = 0.1, paience = 5, verbose = True)

for epoch in range(10):
    train(...)
    val_loss = validate(...)
    # Note that step should be called after validate()
    scheduler.step(val_loss)
```

- 위 코드를 보면 ① 먼저 model을 선언한 다음 ② 그 모델을 optimizer에 할당합니다. 그 다음 ③ 스케쥴러에 optimizer를 할당합니다.
- 위 순서에 따라 model, optimizer, scheduler가 모두 엮이게 됩니다.
- 스케쥴러 업데이트는 보통 **validation 과정을 거친 후** 사용합니다. 위 코드와 같이 validation loss를 `scheduler.step()`의 입력으로 주면 그 loss값과 scheduler 선언 시 사용한 옵션들을 이용하여 learning_rate를 dynamic하게 조절할 수 있습니다.

- **이미지 변형함수**
    
    **transforms.ToPILImage()** - csv 파일로 데이터셋을 받을 경우, PIL image로 바꿔준다.
    
    **transforms.CenterCrop(size)** - 가운데 부분을 size 크기로 자른다.
    
    **transforms.Grayscale(num_output_channels=1)** - grayscale로 변환한다.
    
    **transforms.RandomAffine(degrees)** - 랜덤으로 affine 변형을 한다.
    
    **transforms.RandomCrop(size)** -이미지를 랜덤으로 아무데나 잘라 size 크기로 출력한다.
    
    **transforms.RandomResizedCrop(size)** - 이미지 사이즈를 size로 변경한다
    
    **transforms.Resize(*size*)** - 이미지 사이즈를 size로 변경한다
    
    **transforms.RandomRotation(degrees)** 이미지를 랜덤으로 degrees 각도로 회전한다.
    
    **transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333))** - 이미지를 랜덤으로 변형한다.
    
    **transforms.RandomVerticalFlip(p=0.5)** - 이미지를 랜덤으로 수직으로 뒤집는다. p =0이면 뒤집지 않는다.
    
    **transforms.RandomHorizontalFlip(p=0.5)** - 이미지를 랜덤으로 수평으로 뒤집는다.
    
    **transforms.ToTensor()** - 이미지 데이터를 tensor로 바꿔준다.
    
    **transforms.Normalize(mean, std, inplace=False)** - 이미지를 정규화한다.
    

`make_grid`는 image grid를 포함한 tensor를 반환해준다. batch의 갯수만큼 포함해서 하나의 이미지로 출력해준다.

- **모델의 가중치 저장함수**
    - `torch.save`:
        - **객체를 디스크에 저장**합니다. `pickle` 모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다.
        - **전체 모델을 저장**하거나, **모델의 `state_dict`를 저장** 할 때 사용합니다.
        - ****torch.save(object, path)****
        - `object`: 저장할 모델 객체
        - `path`: 저장할 위치 + 파일명
    - `torch.load`: `pickle` 모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다.
    - `torch.nn.Module.load_state_dict`: 역직렬화된 *state_dict*를 사용, 모델의 매개변수들을 불러옵니다. *state_dict*는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python 사전(dict) 객체입니다. state_dict란 torch.nn.Module에서 모델로 학습할 때 각 layer마다 텐서로 매핑되는 매개변수(예를 들어 가중치, 편향과 같은)를 python dictionary 타입으로 저장한 객체이다.

**nn.Sequantial**을 사용하면 **init**() 에서 사용할 네트워크 모델들을 정의 해줄 뿐만 아니라, forward() 함수에서 구현될 순전파를 Layer 형태로 보다 가독성이 뛰어나게 코드를 작성할 수 있습니다. Layer가 복잡해질수록 nn.Sequential은 그 효가가 뛰어납니다.

**torch.nn.Linear**(**in_features**,**out_features**,**bias** = True, **device** = None,**dtype** = None)

Parameter를 보면

**in_features**는 input sample의 size

**out_features**는 output sample의 size

**bias**는 만일 False로 설정되어 있으면 layer는 bias를 학습하지 않는다. 기본값은 True이다.

**device**는 CPU, GPU 중 고르는 거고

**dtype**은 자료형의 타입을 정하는 것 같다.

***문법 참고사이트**

[**https://gaussian37.github.io/dl-pytorch-snippets/**](https://gaussian37.github.io/dl-pytorch-snippets/)
