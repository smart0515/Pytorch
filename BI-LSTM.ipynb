{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  BI-LSTM\n",
    "- BI-LSTM은 정보를 역방향으로 전달하는 히든 레이어를 추가하여 이러한 정보를 보다 유연하게 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:56:38.722418Z",
     "start_time": "2023-02-01T03:56:35.591310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x251b00f0850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from string import punctuation  # 구두점 제거에 이용\n",
    "from collections import Counter # 데이터의 개수를 셀 때 유용한 클래스\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "# 신경망 생성 및 학습에 도움을 주는 모듈/클래스\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset # 데이터셋을 불러온 뒤, 순회할 수 있는 객체\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # GPU 가능시 device='cuda'\n",
    "torch.manual_seed(123) # random seed를 고정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:56:38.737417Z",
     "start_time": "2023-02-01T03:56:38.723418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 데이터셋 로딩과 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:56:38.857418Z",
     "start_time": "2023-02-01T03:56:38.738418Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torchtext import (data, datasets) # 텍스트 분류 분석을 위한 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:03.898625Z",
     "start_time": "2023-02-01T03:56:38.858418Z"
    }
   },
   "outputs": [],
   "source": [
    "# 필드를 통해 앞으로 어떤 전처리를 할 것인지를 정의합니다\n",
    "TEXT_FIELD = data.Field(tokenize = data.get_tokenizer(\"basic_english\"), include_lengths = True)\n",
    "LABEL_FIELD = data.LabelField(dtype = torch.float)\n",
    "\n",
    "# 리뷰텍스트와 감성레이블 두개의 필드로 나눈다\n",
    "train_dataset, test_dataset = datasets.IMDB.splits(TEXT_FIELD, LABEL_FIELD) \n",
    "train_dataset, valid_dataset = train_dataset.split(random_state = random.seed(123))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - torchtext.data.Field와 torchtext.data.LabelField의 build_vocab 메서드를 사용해 <br>각각 영화 리뷰 텍스트 데이터셋(IMDb)과 감성 레이블에 대한 사전을 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:04.336156Z",
     "start_time": "2023-02-01T03:59:03.900626Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_VOCABULARY_SIZE = 25000\n",
    "\n",
    "# build_vocab 메서드로 단어사전을 구성\n",
    "TEXT_FIELD.build_vocab(train_dataset, max_size = MAX_VOCABULARY_SIZE)  \n",
    "LABEL_FIELD.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bucketing은 주어진 문장의 길이에 따라 데이터를 그룹화하여 padding을 적용하는 기법이다**\n",
    "\n",
    "- 길이가 천차만별인 데이터들을 하나의 batch내에 넣는다면 가장 큰 데이터의 길이만큼  <br>padding이 되어야하므로 쓸데없이 0으로 차있게 돼 학습에 시간이 오래걸린다. <br><br>\n",
    "\n",
    "- 하지만 Bucketing은 길이가 비슷한 데이터들끼리 하나의 batch로 만들어 padding을 최소화시킨다. <br><br>\n",
    "\n",
    "- 이 기법은 모델의 학습 시간을 단축하기 위해 고안되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bucketiterator**\n",
    "- Pytorch의 dataloader와 비슷한 역할을 한다. <br><br>\n",
    "- 하지만 dataloader 와 다르게 비슷한 길이의 문장들끼리 batch를 만들기 때문에<br> padding의 개수를 최소화할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:04.351163Z",
     "start_time": "2023-02-01T03:59:04.337166Z"
    }
   },
   "outputs": [],
   "source": [
    "B_SIZE = 64  # batch size\n",
    "\n",
    "train_data_iterator, valid_data_iterator, test_data_iterator = \\\n",
    "data.BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset, test_dataset), \n",
    "    batch_size = B_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PackedSequence**\n",
    "- NLP에서 매 배치마다 고정된 문장의 길이로 만들어주기 위해서 <pad.> 토큰을 넣어야하는데, <br>\n",
    "문장의 길이별로 정렬해주지 않고 수행을 하면 <pad.> 토큰까지 연산을 하게 된다. <br>\n",
    "따라서 이를 계산하지않고 효율적으로 진행하기 위해 병렬처리를 하려고한다. <br>\n",
    "이를 통해 sequence를 마치 상삼각행렬을 좌우반전 시킨형태로 정렬시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:04.366163Z",
     "start_time": "2023-02-01T03:59:04.352163Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU 환경을 사용하는 경우 pack_padded_sequence 메서드가 작동하려면 다음 함수를 사용 \n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence\n",
    "\n",
    "def cuda_pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True):\n",
    "    lengths = torch.as_tensor(lengths, dtype=torch.int64)\n",
    "    lengths = lengths.cpu()\n",
    "    if enforce_sorted:\n",
    "        sorted_indices = None\n",
    "    else:\n",
    "        lengths, sorted_indices = torch.sort(lengths, descending=True)\n",
    "        sorted_indices = sorted_indices.to(input.device)\n",
    "        batch_dim = 0 if batch_first else 1\n",
    "        input = input.index_select(batch_dim, sorted_indices)\n",
    "\n",
    "    data, batch_sizes = \\\n",
    "    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)\n",
    "    return PackedSequence(data, batch_sizes, sorted_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 모델 인스턴스화 및 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.Embedding**\n",
    "- *num_embeddings* : 임베딩을 할 단어들의 개수. 다시 말해 **단어 집합의 크기**입니다.<br><br>\n",
    "- *embedding_dim* : **임베딩 할 벡터의 차원**입니다. 사용자가 정해주는 하이퍼파라미터입니다.<br><br>\n",
    "- *padding_idx* : 선택적으로 사용하는 인자입니다. **패딩을 위한 토큰의 인덱스**를 알려줍니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nn.LSTM**\n",
    "- *input_size* -- **The number of expected features in the input x**<br><br>\n",
    "\n",
    "- *hidden_size* -- **The number of features in the hidden state h**<br><br>\n",
    "\n",
    "- *num_layers* -- **Number of recurrent layers**. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, <br>with the second LSTM taking in outputs of the first LSTM and computing the final results.<br> Default: 1<br><br>\n",
    "\n",
    "- *bias* -- **If False**, then the layer **does not use bias weights b_ih and b_hh**.<br> Default: True<br><br>\n",
    "\n",
    "- *batch_first* -- **If True**, then the **input and output tensors are provided as (batch, seq, feature)** instead of (seq, batch, feature). <br>Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False<br><br>\n",
    "\n",
    "- *dropout* -- **If non-zero**, **introduces a Dropout layer on the outputs of each LSTM layer except the last layer**, with dropout probability <br>equal to dropout.<br> Default: 0<br><br>\n",
    "\n",
    "- *bidirectional* -- **If True**, **becomes a bidirectional LSTM**.<br> Default: False<br><br>\n",
    "\n",
    "- *proj_size* -- **If > 0**, will use LSTM with **projections of corresponding size**.<br> Default: 0<br><br><br>\n",
    "from pytorch document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:04.381163Z",
     "start_time": "2023-02-01T03:59:04.367165Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dimension, hidden_dimension, output_dimension, dropout, pad_index):\n",
    "        super().__init__()\n",
    "        # 임베딩 테이블을 생성합니다, embedding_dimension만큼 축소\n",
    "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_dimension, padding_idx = pad_index)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dimension, \n",
    "                                  hidden_dimension, \n",
    "                                  num_layers=1,  # 재귀 층의 개수, stacked lstm의 경우에는 2개 이상\n",
    "                                  bidirectional=True,  # numdirection=2\n",
    "                                  dropout=dropout)\n",
    "        self.fc_layer = nn.Linear(hidden_dimension * 2, output_dimension)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, sequence, sequence_lengths=None):\n",
    "        if sequence_lengths is None:\n",
    "            sequence_lengths = torch.LongTensor([len(sequence)]) # 64비트의 부호 있는 정수는 torch.LongTensor를 사용합니다\n",
    "        \n",
    "        # sequence := (sequence_length, batch_size)\n",
    "        embedded_output = self.dropout_layer(self.embedding_layer(sequence))\n",
    "        \n",
    "        \n",
    "        # embedded_output := (sequence_length, batch_size, embedding_dimension)\n",
    "        # GPU 환경 사용시 위에서 정의한 함수를 사용\n",
    "        if torch.cuda.is_available():  \n",
    "            # PackedSequence object를 얻는다\n",
    "            packed_embedded_output = cuda_pack_padded_sequence(embedded_output, sequence_lengths) \n",
    "        else:\n",
    "            # PackedSequence object를 얻는다\n",
    "            packed_embedded_output = nn.utils.rnn.pack_padded_sequence(embedded_output, sequence_lengths)\n",
    "        \n",
    "        packed_output, (hidden_state, cell_state) = self.lstm_layer(packed_embedded_output)\n",
    "        # hidden_state := (num_layers * num_directions, batch_size, hidden_dimension)\n",
    "        # cell_state := (num_layers * num_directions, batch_size, hidden_dimension)\n",
    "        \n",
    "        op, op_lengths = nn.utils.rnn.pad_packed_sequence(packed_output) # 패킹된 문장을 다시 unpack\n",
    "        # op := (sequence_length, batch_size, hidden_dimension * num_directions)\n",
    "        \n",
    "        hidden_output = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1) \n",
    "        # bidirectional=True이므로 이전 hidden state 2개를 호출하여 연결 후 사용한다\n",
    "        # hidden_output := (batch_size, hidden_dimension * num_directions)\n",
    "        \n",
    "        return self.fc_layer(hidden_output)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:05.978520Z",
     "start_time": "2023-02-01T03:59:04.382163Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebdl\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIMENSION = len(TEXT_FIELD.vocab)\n",
    "EMBEDDING_DIMENSION = 100\n",
    "HIDDEN_DIMENSION = 32\n",
    "OUTPUT_DIMENSION = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_INDEX = TEXT_FIELD.vocab.stoi[TEXT_FIELD.pad_token]\n",
    "\n",
    "lstm_model = LSTM(INPUT_DIMENSION, \n",
    "            EMBEDDING_DIMENSION, \n",
    "            HIDDEN_DIMENSION, \n",
    "            OUTPUT_DIMENSION, \n",
    "            DROPOUT, \n",
    "            PAD_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사전에 두개의 특수 토큰 추가<br>\n",
    "- 하나는 사전에 없는 단어를 위한 unknown토큰,다른 하나는 시퀀스 패딩을 위해 추가디는 padding 토큰이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:05.993520Z",
     "start_time": "2023-02-01T03:59:05.979520Z"
    }
   },
   "outputs": [],
   "source": [
    "UNK_INDEX = TEXT_FIELD.vocab.stoi[TEXT_FIELD.unk_token] # 현재 단어 집합의 단어와 맵핑된 고유한 정수를 출력할 수 있다\n",
    "\n",
    "lstm_model.embedding_layer.weight.data[UNK_INDEX] = torch.zeros(EMBEDDING_DIMENSION) # 가중치 초기화\n",
    "lstm_model.embedding_layer.weight.data[PAD_INDEX] = torch.zeros(EMBEDDING_DIMENSION) # 가중치 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:06.008520Z",
     "start_time": "2023-02-01T03:59:05.994521Z"
    }
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(lstm_model.parameters()) # optimizer = 'Adam'\n",
    "# BCELoss에서는 CrossEntropyLoss와 같이 softmax를 포함한 것이 아닌, Cross Entropy만 구합니다\n",
    "# (Sigmoid + BCELoss) 따라서 따로 sigmoid 나 softmax를 해줄 필요가 없습니다\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "lstm_model = lstm_model.to(device)\n",
    "loss_func = loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:06.023520Z",
     "start_time": "2023-02-01T03:59:06.010520Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_metric(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns 0-1 accuracy for the given set of predictions and ground truth\n",
    "    \"\"\"\n",
    "    # 예측을 0 또는 1로 반올림\n",
    "    rounded_predictions = torch.round(torch.sigmoid(predictions))\n",
    "    success = (rounded_predictions == ground_truth).float() # 나눗셈을 위해 float로 변환\n",
    "    accuracy = success.sum() / len(success)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:06.038520Z",
     "start_time": "2023-02-01T03:59:06.024521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel=lstm\\ndata_iterator=train_data_iterator\\noptim=Adam\\nloss_func=BCEWithLogitsLoss\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(model, data_iterator, optim, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.train()\n",
    "    \n",
    "    for curr_batch in data_iterator:\n",
    "        optim.zero_grad()\n",
    "        sequence, sequence_lengths = curr_batch.text\n",
    "        preds = lstm_model(sequence, sequence_lengths).squeeze(1)\n",
    "        \n",
    "        loss_curr = loss_func(preds, curr_batch.label)\n",
    "        accuracy_curr = accuracy_metric(preds, curr_batch.label)\n",
    "        \n",
    "        loss_curr.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss += loss_curr.item()\n",
    "        accuracy += accuracy_curr.item()\n",
    "        \n",
    "    return loss/len(data_iterator), accuracy/len(data_iterator)\n",
    "\n",
    "'''\n",
    "model=lstm\n",
    "data_iterator=train_data_iterator\n",
    "optim=Adam\n",
    "loss_func=BCEWithLogitsLoss\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:06.053520Z",
     "start_time": "2023-02-01T03:59:06.044520Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(model, data_iterator, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for curr_batch in data_iterator:\n",
    "            sequence, sequence_lengths = curr_batch.text\n",
    "            preds = model(sequence, sequence_lengths).squeeze(1)\n",
    "            \n",
    "            loss_curr = loss_func(preds, curr_batch.label)\n",
    "            accuracy_curr = accuracy_metric(preds, curr_batch.label)\n",
    "\n",
    "            loss += loss_curr.item()\n",
    "            accuracy += accuracy_curr.item()\n",
    "        \n",
    "    return loss/len(data_iterator), accuracy/len(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T03:59:59.127350Z",
     "start_time": "2023-02-01T03:59:06.054520Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 1 | time elapsed: 5.629080057144165s\n",
      "training loss: 0.687 | training accuracy: 55.23%\n",
      "validation loss: 0.669 |  validation accuracy: 58.54%\n",
      "\n",
      "epoch number: 2 | time elapsed: 5.211093425750732s\n",
      "training loss: 0.657 | training accuracy: 61.17%\n",
      "validation loss: 0.868 |  validation accuracy: 58.99%\n",
      "\n",
      "epoch number: 3 | time elapsed: 5.304181814193726s\n",
      "training loss: 0.580 | training accuracy: 69.28%\n",
      "validation loss: 0.751 |  validation accuracy: 63.22%\n",
      "\n",
      "epoch number: 4 | time elapsed: 5.317663669586182s\n",
      "training loss: 0.516 | training accuracy: 75.04%\n",
      "validation loss: 0.764 |  validation accuracy: 69.41%\n",
      "\n",
      "epoch number: 5 | time elapsed: 5.233748912811279s\n",
      "training loss: 0.466 | training accuracy: 78.37%\n",
      "validation loss: 0.650 |  validation accuracy: 71.18%\n",
      "\n",
      "epoch number: 6 | time elapsed: 5.318321228027344s\n",
      "training loss: 0.433 | training accuracy: 80.42%\n",
      "validation loss: 0.622 |  validation accuracy: 75.88%\n",
      "\n",
      "epoch number: 7 | time elapsed: 5.384981155395508s\n",
      "training loss: 0.402 | training accuracy: 82.23%\n",
      "validation loss: 0.690 |  validation accuracy: 73.85%\n",
      "\n",
      "epoch number: 8 | time elapsed: 5.18363618850708s\n",
      "training loss: 0.415 | training accuracy: 81.03%\n",
      "validation loss: 0.623 |  validation accuracy: 75.63%\n",
      "\n",
      "epoch number: 9 | time elapsed: 5.212568759918213s\n",
      "training loss: 0.386 | training accuracy: 83.47%\n",
      "validation loss: 0.547 |  validation accuracy: 78.59%\n",
      "\n",
      "epoch number: 10 | time elapsed: 5.202539443969727s\n",
      "training loss: 0.353 | training accuracy: 85.13%\n",
      "validation loss: 0.455 |  validation accuracy: 81.35%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    time_start = time.time()\n",
    "    \n",
    "    training_loss, train_accuracy = train(lstm_model, train_data_iterator, optim, loss_func)\n",
    "    validation_loss, validation_accuracy = validate(lstm_model, valid_data_iterator, loss_func)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_delta = time_end - time_start \n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(lstm_model.state_dict(), 'lstm_model.pt')\n",
    "    \n",
    "    print(f'epoch number: {ep+1} | time elapsed: {time_delta}s')\n",
    "    print(f'training loss: {training_loss:.3f} | training accuracy: {train_accuracy*100:.2f}%')\n",
    "    print(f'validation loss: {validation_loss:.3f} |  validation accuracy: {validation_accuracy*100:.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련과 검증셋의 정확도가 비슷한 속도로 증가하는 것으로 보아 드롭아웃이 과적합을 제어하는 것으로 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T04:00:02.015406Z",
     "start_time": "2023-02-01T03:59:59.128352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.499 | test accuracy: 80.45%\n"
     ]
    }
   ],
   "source": [
    "# 이전 단계에서 가장 성능이 좋은 모델을 저장, 이 모델을 로딩해서 테스트셋에서 검증\n",
    "lstm_model.load_state_dict(torch.load('lstm_model.pt'))\n",
    "\n",
    "test_loss, test_accuracy = validate(lstm_model, test_data_iterator, loss_func)\n",
    "\n",
    "print(f'test loss: {test_loss:.3f} | test accuracy: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T04:00:02.030417Z",
     "start_time": "2023-02-01T04:00:02.016405Z"
    }
   },
   "outputs": [],
   "source": [
    "# 감성 추론 함수를 정의\n",
    "def sentiment_inference(model, sentence):\n",
    "    model.eval()\n",
    "    \n",
    "    # text transformations\n",
    "    tokenized = data.get_tokenizer(\"basic_english\")(sentence)\n",
    "    tokenized = [TEXT_FIELD.vocab.stoi[t] for t in tokenized]\n",
    "    \n",
    "    # model inference\n",
    "    model_input = torch.LongTensor(tokenized).to(device)\n",
    "    model_input = model_input.unsqueeze(1)\n",
    "    \n",
    "    pred = torch.sigmoid(model(model_input))\n",
    "    \n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "긍정은 1에 가깝게 부정은 0에 가깝게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T04:00:02.060418Z",
     "start_time": "2023-02-01T04:00:02.031418Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0925179272890091\n",
      "0.00595007324591279\n",
      "0.07338246703147888\n",
      "0.5898579955101013\n",
      "0.9808263778686523\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_inference(lstm_model, \"This film is horrible\"))\n",
    "print(sentiment_inference(lstm_model, \"Director tried too hard but this film is bad\"))\n",
    "print(sentiment_inference(lstm_model, \"Decent movie, although could be shorter\"))\n",
    "print(sentiment_inference(lstm_model, \"This film will be houseful for weeks\"))\n",
    "print(sentiment_inference(lstm_model, \"I loved the movie, every part of it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
